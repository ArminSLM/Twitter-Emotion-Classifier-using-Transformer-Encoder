{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUGWNtOa787w",
        "outputId": "15e9bbb8-2f83-4f5b-b8a2-37032ec64fd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lfgj7hY_S7l",
        "outputId": "e3dfb5f9-6448-430a-aff8-a569ad214aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading Twitter-specific GloVe embeddings...\n",
            "Loading Twitter GloVe (100d)...\n",
            "Loaded 1193513 vectors with dimension 100\n",
            "Vocabulary Size: 13446\n",
            "Optimal Sequence Length: 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 01\n",
            "Train Loss: 1.7466 | Val Loss: 1.6622\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.3142    0.4938    0.3840      1296\n",
            "       worry     0.3399    0.1721    0.2285      1296\n",
            "   happiness     0.2699    0.2562    0.2629      1296\n",
            "     sadness     0.2308    0.0023    0.0046      1296\n",
            "        love     0.4693    0.5363    0.5005      1296\n",
            "    surprise     0.1942    0.3534    0.2506      1296\n",
            "\n",
            "    accuracy                         0.3023      7776\n",
            "   macro avg     0.3030    0.3023    0.2719      7776\n",
            "weighted avg     0.3030    0.3023    0.2719      7776\n",
            "\n",
            "\n",
            "Epoch 02\n",
            "Train Loss: 1.6921 | Val Loss: 1.6194\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.3077    0.4383    0.3616      1296\n",
            "       worry     0.2178    0.2060    0.2117      1296\n",
            "   happiness     0.3515    0.2840    0.3141      1296\n",
            "     sadness     0.3216    0.4653    0.3803      1296\n",
            "        love     0.5738    0.4498    0.5043      1296\n",
            "    surprise     0.1841    0.1088    0.1368      1296\n",
            "\n",
            "    accuracy                         0.3254      7776\n",
            "   macro avg     0.3261    0.3254    0.3181      7776\n",
            "weighted avg     0.3261    0.3254    0.3181      7776\n",
            "\n",
            "\n",
            "Epoch 03\n",
            "Train Loss: 1.6790 | Val Loss: 1.6074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.3048    0.5941    0.4029      1296\n",
            "       worry     0.0000    0.0000    0.0000      1296\n",
            "   happiness     0.2776    0.5849    0.3765      1296\n",
            "     sadness     0.3628    0.4028    0.3817      1296\n",
            "        love     0.6308    0.3981    0.4882      1296\n",
            "    surprise     0.1947    0.0394    0.0655      1296\n",
            "\n",
            "    accuracy                         0.3365      7776\n",
            "   macro avg     0.2951    0.3365    0.2858      7776\n",
            "weighted avg     0.2951    0.3365    0.2858      7776\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 04\n",
            "Train Loss: 1.6685 | Val Loss: 1.6005\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.3027    0.5355    0.3867      1296\n",
            "       worry     0.2788    0.3796    0.3215      1296\n",
            "   happiness     0.2990    0.5895    0.3968      1296\n",
            "     sadness     0.4741    0.1412    0.2176      1296\n",
            "        love     0.6280    0.3596    0.4573      1296\n",
            "    surprise     0.3429    0.0093    0.0180      1296\n",
            "\n",
            "    accuracy                         0.3358      7776\n",
            "   macro avg     0.3876    0.3358    0.2997      7776\n",
            "weighted avg     0.3876    0.3358    0.2997      7776\n",
            "\n",
            "\n",
            "Epoch 05\n",
            "Train Loss: 1.6596 | Val Loss: 1.5977\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.2787    0.6767    0.3948      1296\n",
            "       worry     0.2988    0.3248    0.3113      1296\n",
            "   happiness     0.2978    0.4313    0.3523      1296\n",
            "     sadness     0.5270    0.0980    0.1653      1296\n",
            "        love     0.6172    0.4105    0.4930      1296\n",
            "    surprise     0.1708    0.0316    0.0534      1296\n",
            "\n",
            "    accuracy                         0.3288      7776\n",
            "   macro avg     0.3650    0.3288    0.2950      7776\n",
            "weighted avg     0.3650    0.3288    0.2950      7776\n",
            "\n",
            "\n",
            "Epoch 06\n",
            "Train Loss: 1.6424 | Val Loss: 1.5706\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.3151    0.5787    0.4081      1296\n",
            "       worry     0.2559    0.1752    0.2080      1296\n",
            "   happiness     0.3446    0.4344    0.3843      1296\n",
            "     sadness     0.3751    0.4691    0.4169      1296\n",
            "        love     0.6227    0.4151    0.4981      1296\n",
            "    surprise     0.2359    0.0710    0.1091      1296\n",
            "\n",
            "    accuracy                         0.3573      7776\n",
            "   macro avg     0.3582    0.3573    0.3374      7776\n",
            "weighted avg     0.3582    0.3573    0.3374      7776\n",
            "\n",
            "\n",
            "Epoch 07\n",
            "Train Loss: 1.6358 | Val Loss: 1.5779\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.2822    0.6752    0.3980      1296\n",
            "       worry     0.2567    0.1412    0.1822      1296\n",
            "   happiness     0.3303    0.5023    0.3985      1296\n",
            "     sadness     0.4329    0.3233    0.3701      1296\n",
            "        love     0.6307    0.4059    0.4939      1296\n",
            "    surprise     0.2011    0.0293    0.0512      1296\n",
            "\n",
            "    accuracy                         0.3462      7776\n",
            "   macro avg     0.3556    0.3462    0.3157      7776\n",
            "weighted avg     0.3556    0.3462    0.3157      7776\n",
            "\n",
            "\n",
            "Epoch 08\n",
            "Train Loss: 1.6315 | Val Loss: 1.5690\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.3286    0.5370    0.4077      1296\n",
            "       worry     0.2494    0.3410    0.2881      1296\n",
            "   happiness     0.3562    0.3889    0.3718      1296\n",
            "     sadness     0.4221    0.3472    0.3810      1296\n",
            "        love     0.5620    0.4931    0.5253      1296\n",
            "    surprise     0.2463    0.0509    0.0844      1296\n",
            "\n",
            "    accuracy                         0.3597      7776\n",
            "   macro avg     0.3608    0.3597    0.3431      7776\n",
            "weighted avg     0.3608    0.3597    0.3431      7776\n",
            "\n",
            "\n",
            "Epoch 09\n",
            "Train Loss: 1.6280 | Val Loss: 1.5706\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.3121    0.5748    0.4046      1296\n",
            "       worry     0.2831    0.0841    0.1297      1296\n",
            "   happiness     0.3501    0.4877    0.4076      1296\n",
            "     sadness     0.4324    0.2963    0.3516      1296\n",
            "        love     0.6013    0.4444    0.5111      1296\n",
            "    surprise     0.2225    0.2323    0.2273      1296\n",
            "\n",
            "    accuracy                         0.3533      7776\n",
            "   macro avg     0.3669    0.3533    0.3386      7776\n",
            "weighted avg     0.3669    0.3533    0.3386      7776\n",
            "\n",
            "\n",
            "Epoch 10\n",
            "Train Loss: 1.6253 | Val Loss: 1.5649\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.2941    0.6559    0.4061      1296\n",
            "       worry     0.2979    0.1860    0.2290      1296\n",
            "   happiness     0.3778    0.3997    0.3885      1296\n",
            "     sadness     0.3898    0.4082    0.3988      1296\n",
            "        love     0.6290    0.4213    0.5046      1296\n",
            "    surprise     0.2287    0.0849    0.1238      1296\n",
            "\n",
            "    accuracy                         0.3593      7776\n",
            "   macro avg     0.3696    0.3593    0.3418      7776\n",
            "weighted avg     0.3696    0.3593    0.3418      7776\n",
            "\n",
            "\n",
            "Epoch 11\n",
            "Train Loss: 1.6214 | Val Loss: 1.5642\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.2890    0.6667    0.4032      1296\n",
            "       worry     0.2723    0.2469    0.2590      1296\n",
            "   happiness     0.3515    0.4529    0.3958      1296\n",
            "     sadness     0.4123    0.2485    0.3101      1296\n",
            "        love     0.6191    0.4352    0.5111      1296\n",
            "    surprise     0.2651    0.0509    0.0854      1296\n",
            "\n",
            "    accuracy                         0.3502      7776\n",
            "   macro avg     0.3682    0.3502    0.3274      7776\n",
            "weighted avg     0.3682    0.3502    0.3274      7776\n",
            "\n",
            "\n",
            "Epoch 12\n",
            "Train Loss: 1.6133 | Val Loss: 1.5530\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.2980    0.6227    0.4031      1296\n",
            "       worry     0.2917    0.1767    0.2201      1296\n",
            "   happiness     0.3500    0.5015    0.4123      1296\n",
            "     sadness     0.4088    0.4066    0.4077      1296\n",
            "        love     0.6380    0.4174    0.5047      1296\n",
            "    surprise     0.2734    0.0610    0.0997      1296\n",
            "\n",
            "    accuracy                         0.3643      7776\n",
            "   macro avg     0.3767    0.3643    0.3413      7776\n",
            "weighted avg     0.3767    0.3643    0.3413      7776\n",
            "\n",
            "\n",
            "Early stopping after 4 epochs without improvement\n",
            "\n",
            "Final Test Accuracy: 0.3522\n",
            "\n",
            "Text: OMG just got tickets for the concert!!! ğŸ˜ #excited\n",
            "Processed: omg got ticket concert smilingfacewithheartey excit\n",
            "Predicted Emotion: happiness (0.24)\n",
            "============================================================\n",
            "\n",
            "Text: This service is terrible! Worst experience ever ğŸ˜ \n",
            "Processed: servic terribl worst experi ever angryfac\n",
            "Predicted Emotion: sadness (0.36)\n",
            "============================================================\n",
            "\n",
            "Text: Feeling so anxious about the interview tomorrow...\n",
            "Processed: feel anxiou interview tomorrow\n",
            "Predicted Emotion: sadness (0.36)\n",
            "============================================================\n",
            "\n",
            "Text: Lost my pet today. I'm completely heartbroken ğŸ’”\n",
            "Processed: lost pet today complet heartbroken brokenheart\n",
            "Predicted Emotion: sadness (0.42)\n",
            "============================================================\n",
            "\n",
            "Text: What a beautiful morning! ğŸŒ #blessed\n",
            "Processed: beauti morn sunwithfac bless\n",
            "Predicted Emotion: love (0.58)\n",
            "============================================================\n",
            "\n",
            "Text: lol that's hilarious ğŸ˜‚\n",
            "Processed: lol that' hilari facewithtearsofjoy\n",
            "Predicted Emotion: neutral (0.23)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import emoji\n",
        "import kagglehub\n",
        "\n",
        "nltk.download(['punkt', 'stopwords', 'wordnet', 'omw-1.4'], quiet=True)\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "# stopwords  Ùˆ lemmatizer: Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ Ø±ÛŒØ´Ù‡ Ø§ÙˆÙ†Ù‡Ø§ \n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "# Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÛŒ Ø¨Ø±Ø¯Ø§Ø± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ùˆ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø± Ú©Ù„Ù…Ø§ØªÛŒ Ú©Ù‡ Ø¯Ø± Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ Ù†ÛŒØ³ØªÙ†\n",
        "class EnhancedVocab(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.unk_vector = None\n",
        "        self.unk_std = None\n",
        "\n",
        "def twitter_preprocessor(text):\n",
        "    # ØªØ¨Ø¯ÛŒÙ„ Ø§ÛŒÙ…ÙˆØ¬ÛŒ\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "\n",
        "    # Ø­Ø°Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ùˆ Ù†Ø§Ù… Ú©Ø§Ø±Ø¨Ø±ÛŒâ€ŒÙ‡Ø§\n",
        "    text = re.sub(r'http\\S+|@\\w+', '', text)\n",
        "\n",
        "    # Ø­ÙØ¸ Ù‡Ø´ØªÚ¯â€ŒÙ‡Ø§ (ÙÙ‚Ø· Ø­Ø°Ù #)\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "\n",
        "    # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ (Ø¨Ù‡ Ø¬Ø² ØªÚ© Ø¢Ù¾ÙˆØ³ØªØ±ÙˆÙ)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s']\", '', text)\n",
        "\n",
        "    # ØªØµØ­ÛŒØ­ ØªÚ©Ø±Ø§Ø± Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§\n",
        "    text = re.sub(r'(.)\\1{3,}', r'\\1', text)\n",
        "\n",
        "    # Ù‡Ù…Ù‡ Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ ØµÙˆØ±Øª Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú© Ù†ÙˆØ´ØªÙ‡ Ø´ÙˆÙ†Ø¯\n",
        "    tokens = tokenizer.tokenize(text.lower())\n",
        "\n",
        "    processed_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in STOPWORDS and len(token) > 2:\n",
        "            lem = lemmatizer.lemmatize(token)\n",
        "            stem = stemmer.stem(lem)\n",
        "            processed_tokens.append(stem)\n",
        "    # Ø§Ú¯Ø± Ø¯ÛŒÚ¯Ù‡ Ú©Ù„Ù…Ù‡ Ø§ÛŒ Ø¬Ø² Ù‡ÛŒÚ† Ú©Ø¯ÙˆÙ… Ø§Ø² Ø§ÛŒÙ†Ù‡Ø§ Ù†Ø¨ÙˆØ¯ ØŒ Ø¬Ø² Ø­Ø§Ù„Øª \"Ø·Ø¨ÛŒØ¹ÛŒ\" Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±Ù‡\n",
        "    return ' '.join(processed_tokens) if processed_tokens else 'neutral'\n",
        "\n",
        "dataset_path = kagglehub.dataset_download(\"pashupatigupta/emotion-detection-from-text\")\n",
        "data = pd.read_csv(os.path.join(dataset_path, \"tweet_emotions.csv\"))\n",
        "data['clean_text'] = data['content'].apply(twitter_preprocessor)\n",
        "\n",
        "top_labels = data['sentiment'].value_counts().nlargest(6).index\n",
        "data = data[data['sentiment'].isin(top_labels)].copy()\n",
        "final_label_mapping = {label: idx for idx, label in enumerate(top_labels)}\n",
        "data['label'] = data['sentiment'].map(final_label_mapping)\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_res, y_res = ros.fit_resample(\n",
        "    data[['clean_text']],\n",
        "    data['label']\n",
        ")\n",
        "data_balanced = pd.DataFrame({\n",
        "    'clean_text': X_res['clean_text'],\n",
        "    'label': y_res\n",
        "})\n",
        "\n",
        "# Ø®Ø¨ Ø­Ø§Ù„Ø§ Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø§ 6 Ú©Ù„Ø§Ø³ Ø¨Ø¹Ø¯ Ø§Ø² Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø§ Ø§Ù…Ø§Ø¯Ù‡ Ø´Ø¯\n",
        "\n",
        "GLOVE_DIM = 100  \n",
        "GLOVE_FILE = f\"glove.twitter.27B.{GLOVE_DIM}d.txt\"\n",
        "\n",
        "if not os.path.exists(GLOVE_FILE):\n",
        "    print(\"Downloading Twitter-specific GloVe embeddings...\")\n",
        "    os.system(\"wget https://nlp.stanford.edu/data/glove.twitter.27B.zip\")\n",
        "    os.system(\"unzip -o glove.twitter.27B.zip\")\n",
        "\n",
        "print(f\"Loading Twitter GloVe ({GLOVE_DIM}d)...\")\n",
        "glove_embeddings = {}\n",
        "valid_vectors = []\n",
        "\n",
        "with open(GLOVE_FILE, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            if len(vector) == GLOVE_DIM:\n",
        "                glove_embeddings[word] = vector\n",
        "                valid_vectors.append(vector)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "all_vectors = np.array(valid_vectors)\n",
        "print(f\"Loaded {len(glove_embeddings)} vectors with dimension {GLOVE_DIM}\")\n",
        "\n",
        "# ØªÚ©Ù†ÛŒÚ© Ù„Ø­Ø§Ø¸ ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø± Ú©Ù„Ù…Ø§Øª  \n",
        "def build_twitter_vocab(texts, min_freq=2):\n",
        "    counter = Counter()\n",
        "    for text in texts:\n",
        "        counter.update(text.split())\n",
        "\n",
        "# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø± Ùˆ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¨Ø±Ø§ÛŒ ØªØ®Ù…ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø´Ù†Ø§Ø®ØªØªÙ‡\n",
        "    mean_vector = np.mean(all_vectors, axis=0)\n",
        "    std_vector = np.std(all_vectors, axis=0)\n",
        "\n",
        "    vocab = EnhancedVocab()\n",
        "    vocab.update({\n",
        "        \"[PAD]\": 0,\n",
        "        \"[UNK]\": 1,\n",
        "        \"[CLS]\": 2\n",
        "    })\n",
        "\n",
        "    current_idx = 3\n",
        "    for word, freq in counter.items():\n",
        "        if freq >= min_freq and word not in vocab:\n",
        "            vocab[word] = current_idx\n",
        "            current_idx += 1\n",
        "\n",
        "    vocab.unk_vector = mean_vector\n",
        "    vocab.unk_std = std_vector\n",
        "\n",
        "    return vocab\n",
        "\n",
        "\n",
        "vocab = build_twitter_vocab(data_balanced['clean_text'], min_freq=2)\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "# Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¨Ø§ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø± Ùˆ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú©Ù„ Ú©Ù„Ù…Ø§Øª\n",
        "embedding_matrix = np.zeros((vocab_size, GLOVE_DIM))\n",
        "for word, idx in vocab.items():\n",
        "    if word in glove_embeddings:\n",
        "        embedding_matrix[idx] = glove_embeddings[word]\n",
        "    elif idx not in [0, 1, 2]:\n",
        "        embedding_matrix[idx] = np.random.normal(\n",
        "            loc=vocab.unk_vector,\n",
        "            scale=vocab.unk_std,\n",
        "            size=(GLOVE_DIM,)\n",
        "        )\n",
        "\n",
        "seq_lengths = data_balanced['clean_text'].apply(lambda x: len(x.split()))\n",
        "MAX_LEN = int(np.percentile(seq_lengths, 95))\n",
        "print(f\"Optimal Sequence Length: {MAX_LEN}\")\n",
        "\n",
        "# Ø­Ø§Ù„ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ ØªÙˆÚ©Ù† Ùˆ Ø¹Ø¯Ø¯\n",
        "def text_to_sequence(text, vocab, max_len=MAX_LEN):\n",
        "    tokens = text.split()[:max_len-1]\n",
        "    sequence = [vocab[\"[CLS]\"]] + [\n",
        "        vocab.get(token, vocab[\"[UNK]\"]) for token in tokens\n",
        "    ]\n",
        "    padding = [vocab[\"[PAD]\"]] * (max_len - len(sequence))\n",
        "    return sequence + padding if len(sequence) < max_len else sequence[:max_len]\n",
        "\n",
        "data_balanced['sequence'] = data_balanced['clean_text'].apply(\n",
        "    lambda x: text_to_sequence(x, vocab, MAX_LEN)\n",
        ")\n",
        "\n",
        "\n",
        "# ØªÙ‚Ø³ÛŒÙ… Ø¨Ù†Ø¯ÛŒ Ø¯ÛŒØªØ§Ø³Øª\n",
        "X_train_valid, X_test, y_train_valid, y_test = train_test_split(\n",
        "    np.stack(data_balanced['sequence']),\n",
        "    data_balanced['label'].values,\n",
        "    test_size=0.15,\n",
        "    stratify=data_balanced['label'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_valid,\n",
        "    y_train_valid,\n",
        "    test_size=0.1765,\n",
        "    stratify=y_train_valid,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "class TweetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sequences, labels, augment=False):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.sequences[idx]\n",
        "        label = self.labels[idx]\n",
        " # Ø§Ú¯Ø± ÙØ¹Ø§Ù„ Ø¨ÙˆØ¯ØŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø±Ù†Ø¯ÙˆÙ… ØŒ 90% ØªÙˆÚ©Ù† Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¬Ù…Ù„Ù‡ Ø±Ùˆ Ù†Ú¯Ù‡ Ù…ÛŒØ¯Ø§Ø±Ù‡ \n",
        " # Ùˆ 10 % Ø±Ùˆ Ø­Ø°Ù Ù…ÛŒÚ©Ù†Ù‡ Ùˆ Ø¨Ù‡ Ø¬Ø§Ø´ Ù¾Ø§Ø¯ÛŒÙ†Ú¯ Ù…ÛŒØ²Ø§Ø±Ù‡ ØªØ§ Ø·ÙˆÙ„ Ø¬Ù…Ù„Ù‡ Ø¨Ø§ Ø­Ø¯Ø§Ú©Ø«Ø± Ø¨Ø±Ø³Ù‡\n",
        "        if self.augment and np.random.rand() > 0.5:\n",
        "            mask = np.random.rand(len(seq)) > 0.1\n",
        "            seq = seq[mask]\n",
        "            seq = np.pad(seq, (0, MAX_LEN - len(seq)),\n",
        "                         mode='constant', constant_values=vocab[\"[PAD]\"])\n",
        "\n",
        "        return {\n",
        "            'sequence': torch.tensor(seq, dtype=torch.long),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_dataset = TweetDataset(X_train, y_train, augment=True)\n",
        "valid_dataset = TweetDataset(X_valid, y_valid)\n",
        "test_dataset = TweetDataset(X_test, y_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True\n",
        ")\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True\n",
        ")\n",
        "\n",
        "class TwitterTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=256,\n",
        "                 num_layers=4, num_heads=4, num_classes=6,\n",
        "                 max_len=MAX_LEN, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            vocab_size, embedding_dim, padding_idx=vocab[\"[PAD]\"]\n",
        "        )\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = False\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, embedding_dim)) # Ø§Ù‡Ù…ÛŒØª Ù…ÙˆÙ‚Ø¹ÛŒØª Ù‚Ø±Ø§Ø± Ú¯ÛŒØ±ÛŒ Ú©Ù„Ù…Ø§Øª not good\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=embedding_dim,\n",
        "                nhead=num_heads,\n",
        "                dim_feedforward=hidden_dim,\n",
        "                dropout=dropout,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            ),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(embedding_dim),\n",
        "            nn.Linear(embedding_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size()\n",
        "\n",
        "        token_emb = self.embedding(x)\n",
        "        pos_emb = self.pos_embedding[:, :seq_len, :]\n",
        "        x = self.dropout(token_emb + pos_emb)\n",
        "\n",
        "        padding_mask = (x == self.embedding.weight[vocab[\"[PAD]\"]]).all(dim=-1)\n",
        "\n",
        "        x = self.encoder(x, src_key_padding_mask=padding_mask)\n",
        "\n",
        "        cls_output = x[:, 0, :]\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TwitterTransformer(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=GLOVE_DIM,\n",
        "    hidden_dim=256,\n",
        "    num_layers=4,\n",
        "    num_heads=4,\n",
        "    num_classes=len(final_label_mapping),\n",
        "    max_len=MAX_LEN,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "class_counts = np.bincount(data_balanced['label'])\n",
        "class_weights = torch.tensor(1. / np.sqrt(class_counts), dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', patience=2, factor=0.5, verbose=True\n",
        ")\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "best_f1 = 0\n",
        "patience = 4\n",
        "no_improve = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        sequences = batch['sequence'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(sequences)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            sequences = batch['sequence'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    val_loss /= len(valid_loader)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    scheduler.step(f1)\n",
        "\n",
        "    print(f'\\nEpoch {epoch+1:02}')\n",
        "    print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
        "    print(classification_report(\n",
        "        all_labels, all_preds,\n",
        "        target_names=final_label_mapping.keys(),\n",
        "        digits=4\n",
        "    ))\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_twitter_model.pth')\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(f\"\\nEarly stopping after {patience} epochs without improvement\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(torch.load('best_twitter_model.pth'))\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        sequences = batch['sequence'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs = model(sequences)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f'\\nFinal Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "inv_label_mapping = {v: k for k, v in final_label_mapping.items()}\n",
        "\n",
        "test_samples = [\n",
        "    \"OMG just got tickets for the concert!!! ğŸ˜ #excited\",\n",
        "    \"This service is terrible! Worst experience ever ğŸ˜ \",\n",
        "    \"Feeling so anxious about the interview tomorrow...\",\n",
        "    \"Lost my pet today. I'm completely heartbroken ğŸ’”\",\n",
        "    \"What a beautiful morning! ğŸŒ #blessed\",\n",
        "    \"lol that's hilarious ğŸ˜‚\"\n",
        "]\n",
        "\n",
        "for text in test_samples:\n",
        "    processed = twitter_preprocessor(text)\n",
        "    seq = text_to_sequence(processed, vocab, MAX_LEN)\n",
        "    seq_tensor = torch.tensor(seq).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(seq_tensor)\n",
        "        prob = torch.softmax(output, dim=1)\n",
        "        pred = torch.argmax(prob).item()\n",
        "\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(f\"Processed: {processed}\")\n",
        "    print(f\"Predicted Emotion: {inv_label_mapping[pred]} ({prob.max().item():.2f})\")\n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XMrjWWd8fAv",
        "outputId": "8731528b-7906-495b-c7bd-949e7ccc39fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading GloVe (100d)...\n",
            "Vocabulary Size: 13446\n",
            "Optimal Sequence Length: 13\n",
            "Loading pre-trained model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 01\n",
            "Train Loss: 0.5781 | Acc: 78.56%\n",
            "Val Loss: 1.5825 | Acc: 60.55% | F1: 0.5952\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.4485    0.3565    0.3972      1296\n",
            "       worry     0.4275    0.3480    0.3837      1296\n",
            "   happiness     0.5983    0.6528    0.6244      1296\n",
            "     sadness     0.5851    0.6528    0.6171      1296\n",
            "        love     0.7244    0.7485    0.7362      1296\n",
            "    surprise     0.7594    0.8742    0.8128      1296\n",
            "\n",
            "    accuracy                         0.6055      7776\n",
            "   macro avg     0.5905    0.6055    0.5952      7776\n",
            "weighted avg     0.5905    0.6055    0.5952      7776\n",
            "\n",
            "Model saved with F1: 0.5952\n",
            "\n",
            "Epoch 02\n",
            "Train Loss: 0.5733 | Acc: 78.78%\n",
            "Val Loss: 1.5654 | Acc: 60.79% | F1: 0.5987\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.4544    0.3727    0.4095      1296\n",
            "       worry     0.4334    0.3565    0.3912      1296\n",
            "   happiness     0.6026    0.6528    0.6267      1296\n",
            "     sadness     0.5849    0.6381    0.6103      1296\n",
            "        love     0.7207    0.7546    0.7373      1296\n",
            "    surprise     0.7683    0.8727    0.8172      1296\n",
            "\n",
            "    accuracy                         0.6079      7776\n",
            "   macro avg     0.5940    0.6079    0.5987      7776\n",
            "weighted avg     0.5940    0.6079    0.5987      7776\n",
            "\n",
            "Model saved with F1: 0.5987\n",
            "\n",
            "Epoch 03\n",
            "Train Loss: 0.5726 | Acc: 78.62%\n",
            "Val Loss: 1.5712 | Acc: 60.71% | F1: 0.5974\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.4514    0.3657    0.4041      1296\n",
            "       worry     0.4321    0.3511    0.3874      1296\n",
            "   happiness     0.6011    0.6466    0.6230      1296\n",
            "     sadness     0.5825    0.6512    0.6149      1296\n",
            "        love     0.7177    0.7554    0.7361      1296\n",
            "    surprise     0.7715    0.8727    0.8190      1296\n",
            "\n",
            "    accuracy                         0.6071      7776\n",
            "   macro avg     0.5927    0.6071    0.5974      7776\n",
            "weighted avg     0.5927    0.6071    0.5974      7776\n",
            "\n",
            "\n",
            "Epoch 04\n",
            "Train Loss: 0.5761 | Acc: 78.60%\n",
            "Val Loss: 1.5590 | Acc: 60.65% | F1: 0.5965\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.4443    0.3603    0.3980      1296\n",
            "       worry     0.4280    0.3534    0.3872      1296\n",
            "   happiness     0.6156    0.6412    0.6281      1296\n",
            "     sadness     0.5843    0.6474    0.6142      1296\n",
            "        love     0.7178    0.7577    0.7372      1296\n",
            "    surprise     0.7588    0.8789    0.8144      1296\n",
            "\n",
            "    accuracy                         0.6065      7776\n",
            "   macro avg     0.5915    0.6065    0.5965      7776\n",
            "weighted avg     0.5915    0.6065    0.5965      7776\n",
            "\n",
            "\n",
            "Epoch 05\n",
            "Train Loss: 0.5665 | Acc: 79.07%\n",
            "Val Loss: 1.5667 | Acc: 60.55% | F1: 0.5943\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.4479    0.3380    0.3852      1296\n",
            "       worry     0.4270    0.3542    0.3872      1296\n",
            "   happiness     0.6073    0.6420    0.6242      1296\n",
            "     sadness     0.5821    0.6620    0.6195      1296\n",
            "        love     0.7043    0.7608    0.7315      1296\n",
            "    surprise     0.7674    0.8758    0.8180      1296\n",
            "\n",
            "    accuracy                         0.6055      7776\n",
            "   macro avg     0.5893    0.6055    0.5943      7776\n",
            "weighted avg     0.5893    0.6055    0.5943      7776\n",
            "\n",
            "\n",
            "Epoch 06\n",
            "Train Loss: 0.5657 | Acc: 78.97%\n",
            "Val Loss: 1.5749 | Acc: 60.69% | F1: 0.5981\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.4467    0.3526    0.3941      1296\n",
            "       worry     0.4273    0.3673    0.3950      1296\n",
            "   happiness     0.5980    0.6497    0.6228      1296\n",
            "     sadness     0.5804    0.6543    0.6152      1296\n",
            "        love     0.7316    0.7446    0.7380      1296\n",
            "    surprise     0.7795    0.8727    0.8234      1296\n",
            "\n",
            "    accuracy                         0.6069      7776\n",
            "   macro avg     0.5939    0.6069    0.5981      7776\n",
            "weighted avg     0.5939    0.6069    0.5981      7776\n",
            "\n",
            "\n",
            "Epoch 07\n",
            "Train Loss: 0.5678 | Acc: 78.92%\n",
            "Val Loss: 1.5669 | Acc: 60.71% | F1: 0.5977\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.4479    0.3650    0.4022      1296\n",
            "       worry     0.4276    0.3534    0.3870      1296\n",
            "   happiness     0.6054    0.6451    0.6246      1296\n",
            "     sadness     0.5860    0.6520    0.6172      1296\n",
            "        love     0.7174    0.7523    0.7345      1296\n",
            "    surprise     0.7730    0.8750    0.8208      1296\n",
            "\n",
            "    accuracy                         0.6071      7776\n",
            "   macro avg     0.5929    0.6071    0.5977      7776\n",
            "weighted avg     0.5929    0.6071    0.5977      7776\n",
            "\n",
            "\n",
            "Epoch 08\n",
            "Train Loss: 0.5689 | Acc: 78.86%\n",
            "Val Loss: 1.5684 | Acc: 60.67% | F1: 0.5972\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.4445    0.3711    0.4045      1296\n",
            "       worry     0.4282    0.3449    0.3821      1296\n",
            "   happiness     0.6070    0.6435    0.6247      1296\n",
            "     sadness     0.5869    0.6543    0.6188      1296\n",
            "        love     0.7169    0.7523    0.7342      1296\n",
            "    surprise     0.7702    0.8742    0.8189      1296\n",
            "\n",
            "    accuracy                         0.6067      7776\n",
            "   macro avg     0.5923    0.6067    0.5972      7776\n",
            "weighted avg     0.5923    0.6067    0.5972      7776\n",
            "\n",
            "\n",
            "Early stopping after 6 epochs without improvement\n",
            "\n",
            "Final Test Accuracy: 0.6206\n",
            "\n",
            "Text: OMG just got tickets for the concert!!! ğŸ˜ #excited\n",
            "Processed: omg got ticket concert smilingfacewithheartey excit\n",
            "Predicted Emotion: happiness (0.67)\n",
            "============================================================\n",
            "\n",
            "Text: This service is terrible! Worst experience ever ğŸ˜ \n",
            "Processed: servic terribl worst experi ever angryfac\n",
            "Predicted Emotion: worry (0.95)\n",
            "============================================================\n",
            "\n",
            "Text: Feeling so anxious about the interview tomorrow...\n",
            "Processed: feel anxiou interview tomorrow\n",
            "Predicted Emotion: worry (0.79)\n",
            "============================================================\n",
            "\n",
            "Text: Lost my pet today. I'm completely heartbroken ğŸ’”\n",
            "Processed: lost pet today complet heartbroken brokenheart\n",
            "Predicted Emotion: sadness (1.00)\n",
            "============================================================\n",
            "\n",
            "Text: What a beautiful morning! ğŸŒ #blessed\n",
            "Processed: beauti morn sunwithfac bless\n",
            "Predicted Emotion: happiness (0.48)\n",
            "============================================================\n",
            "\n",
            "Text: lol that's hilarious ğŸ˜‚\n",
            "Processed: lol that' hilari facewithtearsofjoy\n",
            "Predicted Emotion: neutral (0.42)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import emoji\n",
        "import kagglehub\n",
        "\n",
        "nltk.download(['punkt', 'stopwords', 'wordnet', 'omw-1.4'], quiet=True)\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "class EnhancedVocab(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.unk_vector = None\n",
        "        self.unk_std = None\n",
        "\n",
        "def twitter_preprocessor(text):\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "    text = re.sub(r'http\\S+|@\\w+', '', text)\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s']\", '', text)\n",
        "    text = re.sub(r'(.)\\1{3,}', r'\\1', text)\n",
        "\n",
        "    tokens = tokenizer.tokenize(text.lower())\n",
        "    processed_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in STOPWORDS and len(token) > 2:\n",
        "            lem = lemmatizer.lemmatize(token)\n",
        "            stem = stemmer.stem(lem)\n",
        "            processed_tokens.append(stem)\n",
        "\n",
        "    return ' '.join(processed_tokens) if processed_tokens else 'neutral'\n",
        "\n",
        "dataset_path = kagglehub.dataset_download(\"pashupatigupta/emotion-detection-from-text\")\n",
        "data = pd.read_csv(os.path.join(dataset_path, \"tweet_emotions.csv\"))\n",
        "data['clean_text'] = data['content'].apply(twitter_preprocessor)\n",
        "\n",
        "top_labels = data['sentiment'].value_counts().nlargest(6).index\n",
        "data = data[data['sentiment'].isin(top_labels)].copy()\n",
        "final_label_mapping = {label: idx for idx, label in enumerate(top_labels)}\n",
        "data['label'] = data['sentiment'].map(final_label_mapping)\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_res, y_res = ros.fit_resample(data[['clean_text']], data['label'])\n",
        "data_balanced = pd.DataFrame({'clean_text': X_res['clean_text'], 'label': y_res})\n",
        "\n",
        "GLOVE_DIM = 100\n",
        "GLOVE_FILE = f\"glove.twitter.27B.{GLOVE_DIM}d.txt\"\n",
        "\n",
        "if not os.path.exists(GLOVE_FILE):\n",
        "    print(\"Downloading GloVe embeddings...\")\n",
        "    os.system(\"wget https://nlp.stanford.edu/data/glove.twitter.27B.zip\")\n",
        "    os.system(\"unzip -o glove.twitter.27B.zip\")\n",
        "\n",
        "print(f\"Loading GloVe ({GLOVE_DIM}d)...\")\n",
        "glove_embeddings = {}\n",
        "valid_vectors = []\n",
        "\n",
        "with open(GLOVE_FILE, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            if len(vector) == GLOVE_DIM:\n",
        "                glove_embeddings[word] = vector\n",
        "                valid_vectors.append(vector)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "all_vectors = np.array(valid_vectors)\n",
        "\n",
        "def build_twitter_vocab(texts, min_freq=2):\n",
        "    counter = Counter()\n",
        "    for text in texts:\n",
        "        counter.update(text.split())\n",
        "\n",
        "    mean_vector = np.mean(all_vectors, axis=0)\n",
        "    std_vector = np.std(all_vectors, axis=0)\n",
        "\n",
        "    vocab = EnhancedVocab()\n",
        "    vocab.update({\n",
        "        \"[PAD]\": 0,\n",
        "        \"[UNK]\": 1,\n",
        "        \"[CLS]\": 2\n",
        "    })\n",
        "\n",
        "    current_idx = 3\n",
        "    for word, freq in counter.items():\n",
        "        if freq >= min_freq and word not in vocab:\n",
        "            vocab[word] = current_idx\n",
        "            current_idx += 1\n",
        "\n",
        "    vocab.unk_vector = mean_vector\n",
        "    vocab.unk_std = std_vector\n",
        "\n",
        "    return vocab\n",
        "\n",
        "vocab = build_twitter_vocab(data_balanced['clean_text'], min_freq=2)\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, GLOVE_DIM))\n",
        "for word, idx in vocab.items():\n",
        "    if word in glove_embeddings:\n",
        "        embedding_matrix[idx] = glove_embeddings[word]\n",
        "    elif idx not in [0, 1, 2]:\n",
        "        embedding_matrix[idx] = np.random.normal(\n",
        "            loc=vocab.unk_vector,\n",
        "            scale=vocab.unk_std,\n",
        "            size=(GLOVE_DIM,)\n",
        "        )\n",
        "\n",
        "seq_lengths = data_balanced['clean_text'].apply(lambda x: len(x.split()))\n",
        "MAX_LEN = int(np.percentile(seq_lengths, 95))\n",
        "print(f\"Optimal Sequence Length: {MAX_LEN}\")\n",
        "\n",
        "def text_to_sequence(text, vocab, max_len=MAX_LEN):\n",
        "    tokens = text.split()[:max_len-1]\n",
        "    sequence = [vocab[\"[CLS]\"]] + [\n",
        "        vocab.get(token, vocab[\"[UNK]\"]) for token in tokens\n",
        "    ]\n",
        "    padding = [vocab[\"[PAD]\"]] * (max_len - len(sequence))\n",
        "    return sequence + padding if len(sequence) < max_len else sequence[:max_len]\n",
        "\n",
        "data_balanced['sequence'] = data_balanced['clean_text'].apply(\n",
        "    lambda x: text_to_sequence(x, vocab, MAX_LEN)\n",
        ")\n",
        "\n",
        "X_train_valid, X_test, y_train_valid, y_test = train_test_split(\n",
        "    np.stack(data_balanced['sequence']),\n",
        "    data_balanced['label'].values,\n",
        "    test_size=0.15,\n",
        "    stratify=data_balanced['label'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_valid,\n",
        "    y_train_valid,\n",
        "    test_size=0.1765,\n",
        "    stratify=y_train_valid,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "class TweetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sequences, labels, augment=False):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.sequences[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.augment and np.random.rand() > 0.5:\n",
        "            mask = np.random.rand(len(seq)) > 0.1\n",
        "            seq = seq[mask]\n",
        "            seq = np.pad(seq, (0, MAX_LEN - len(seq)),\n",
        "                         mode='constant', constant_values=vocab[\"[PAD]\"])\n",
        "        return {\n",
        "            'sequence': torch.tensor(seq, dtype=torch.long),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_dataset = TweetDataset(X_train, y_train, augment=True)\n",
        "valid_dataset = TweetDataset(X_valid, y_valid)\n",
        "test_dataset = TweetDataset(X_test, y_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True\n",
        ")\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True\n",
        ")\n",
        "\n",
        "class EnhancedTwitterTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=256,\n",
        "                 num_layers=4, num_heads=4, num_classes=6,\n",
        "                 max_len=MAX_LEN, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            vocab_size, embedding_dim, padding_idx=vocab[\"[PAD]\"]\n",
        "        )\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = True  # Ø§Ù…Ú©Ø§Ù† ÙØ§ÛŒÙ†â€ŒØªÛŒÙˆÙ†ÛŒÙ†Ú¯ Ø¬Ø²Ø¦ÛŒ\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_len, embedding_dim))\n",
        "        nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            d_model=embedding_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            encoder_layers,\n",
        "            num_layers=num_layers,\n",
        "            norm=nn.LayerNorm(embedding_dim)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(embedding_dim),\n",
        "            nn.Linear(embedding_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout/2),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            nn.init.constant_(module.bias, 0)\n",
        "            nn.init.constant_(module.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size()\n",
        "\n",
        "        token_emb = self.embedding(x)\n",
        "        pos_emb = self.pos_embedding[:, :seq_len, :]\n",
        "        x = self.dropout(token_emb + pos_emb)\n",
        "\n",
        "        padding_mask = (x == self.embedding.weight[vocab[\"[PAD]\"]]).all(dim=-1)\n",
        "        x = self.encoder(x, src_key_padding_mask=padding_mask)\n",
        "\n",
        "        cls_output = x[:, 0, :]\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_SAVE_PATH = \"twitter_emotion_model.pth\"\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    print(\"Loading pre-trained model...\")\n",
        "    model = EnhancedTwitterTransformer(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=GLOVE_DIM,\n",
        "        hidden_dim=256,\n",
        "        num_layers=4,\n",
        "        num_heads=4,\n",
        "        num_classes=len(final_label_mapping),\n",
        "        max_len=MAX_LEN,\n",
        "        dropout=0.2\n",
        "    ).to(device)\n",
        "    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
        "else:\n",
        "    print(\"Initializing new model...\")\n",
        "    model = EnhancedTwitterTransformer(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=GLOVE_DIM,\n",
        "        hidden_dim=256,\n",
        "        num_layers=4,\n",
        "        num_heads=4,\n",
        "        num_classes=len(final_label_mapping),\n",
        "        max_len=MAX_LEN,\n",
        "        dropout=0.2\n",
        "    ).to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
        "\n",
        "class_counts = np.bincount(data_balanced['label'])\n",
        "class_weights = torch.tensor(1. / np.sqrt(class_counts), dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='max',\n",
        "    patience=3,\n",
        "    factor=0.2,\n",
        "    verbose=True,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "best_f1 = 0\n",
        "patience = 6\n",
        "no_improve = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        sequences = batch['sequence'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(sequences)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_acc = 100. * correct / total\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    val_loss = 0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            sequences = batch['sequence'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += predicted.eq(labels).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_acc = 100. * correct_val / total_val\n",
        "    val_loss /= len(valid_loader)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    scheduler.step(f1)\n",
        "\n",
        "    print(f'\\nEpoch {epoch+1:02}')\n",
        "    print(f'Train Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%')\n",
        "    print(f'Val Loss: {val_loss:.4f} | Acc: {val_acc:.2f}% | F1: {f1:.4f}')\n",
        "    print(classification_report(\n",
        "        all_labels, all_preds,\n",
        "        target_names=final_label_mapping.keys(),\n",
        "        digits=4\n",
        "    ))\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        no_improve = 0\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "        print(f\"Model saved with F1: {f1:.4f}\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(f\"\\nEarly stopping after {patience} epochs without improvement\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        sequences = batch['sequence'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(sequences)\n",
        "        _, predicted = outputs.max(1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f'\\nFinal Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "inv_label_mapping = {v: k for k, v in final_label_mapping.items()}\n",
        "\n",
        "test_samples = [\n",
        "    \"OMG just got tickets for the concert!!! ğŸ˜ #excited\",\n",
        "    \"This service is terrible! Worst experience ever ğŸ˜ \",\n",
        "    \"Feeling so anxious about the interview tomorrow...\",\n",
        "    \"Lost my pet today. I'm completely heartbroken ğŸ’”\",\n",
        "    \"What a beautiful morning! ğŸŒ #blessed\",\n",
        "    \"lol that's hilarious ğŸ˜‚\"\n",
        "]\n",
        "\n",
        "for text in test_samples:\n",
        "    processed = twitter_preprocessor(text)\n",
        "    seq = text_to_sequence(processed, vocab, MAX_LEN)\n",
        "    seq_tensor = torch.tensor(seq).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(seq_tensor)\n",
        "        prob = torch.softmax(output, dim=1)\n",
        "        pred = torch.argmax(prob).item()\n",
        "\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(f\"Processed: {processed}\")\n",
        "    print(f\"Predicted Emotion: {inv_label_mapping[pred]} ({prob.max().item():.2f})\")\n",
        "    print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
