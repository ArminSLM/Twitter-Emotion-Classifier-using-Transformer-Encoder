{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUGWNtOa787w",
        "outputId": "15e9bbb8-2f83-4f5b-b8a2-37032ec64fd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XMrjWWd8fAv",
        "outputId": "8731528b-7906-495b-c7bd-949e7ccc39fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading GloVe (100d)...\n",
            "Vocabulary Size: 13446\n",
            "Optimal Sequence Length: 13\n",
            "Loading pre-trained model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 01\n",
            "Train Loss: 0.5781 | Acc: 78.56%\n",
            "Val Loss: 1.5825 | Acc: 60.55% | F1: 0.5952\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.4485    0.3565    0.3972      1296\n",
            "       worry     0.4275    0.3480    0.3837      1296\n",
            "   happiness     0.5983    0.6528    0.6244      1296\n",
            "     sadness     0.5851    0.6528    0.6171      1296\n",
            "        love     0.7244    0.7485    0.7362      1296\n",
            "    surprise     0.7594    0.8742    0.8128      1296\n",
            "\n",
            "    accuracy                         0.6055      7776\n",
            "   macro avg     0.5905    0.6055    0.5952      7776\n",
            "weighted avg     0.5905    0.6055    0.5952      7776\n",
            "\n",
            "Model saved with F1: 0.5952\n",
            "\n",
            "Epoch 02\n",
            "Train Loss: 0.5733 | Acc: 78.78%\n",
            "Val Loss: 1.5654 | Acc: 60.79% | F1: 0.5987\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.4544    0.3727    0.4095      1296\n",
            "       worry     0.4334    0.3565    0.3912      1296\n",
            "   happiness     0.6026    0.6528    0.6267      1296\n",
            "     sadness     0.5849    0.6381    0.6103      1296\n",
            "        love     0.7207    0.7546    0.7373      1296\n",
            "    surprise     0.7683    0.8727    0.8172      1296\n",
            "\n",
            "    accuracy                         0.6079      7776\n",
            "   macro avg     0.5940    0.6079    0.5987      7776\n",
            "weighted avg     0.5940    0.6079    0.5987      7776\n",
            "\n",
            "Model saved with F1: 0.5987\n",
            "\n",
            "Epoch 03\n",
            "Train Loss: 0.5726 | Acc: 78.62%\n",
            "Val Loss: 1.5712 | Acc: 60.71% | F1: 0.5974\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.4514    0.3657    0.4041      1296\n",
            "       worry     0.4321    0.3511    0.3874      1296\n",
            "   happiness     0.6011    0.6466    0.6230      1296\n",
            "     sadness     0.5825    0.6512    0.6149      1296\n",
            "        love     0.7177    0.7554    0.7361      1296\n",
            "    surprise     0.7715    0.8727    0.8190      1296\n",
            "\n",
            "    accuracy                         0.6071      7776\n",
            "   macro avg     0.5927    0.6071    0.5974      7776\n",
            "weighted avg     0.5927    0.6071    0.5974      7776\n",
            "\n",
            "\n",
            "Epoch 04\n",
            "Train Loss: 0.5761 | Acc: 78.60%\n",
            "Val Loss: 1.5590 | Acc: 60.65% | F1: 0.5965\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.4443    0.3603    0.3980      1296\n",
            "       worry     0.4280    0.3534    0.3872      1296\n",
            "   happiness     0.6156    0.6412    0.6281      1296\n",
            "     sadness     0.5843    0.6474    0.6142      1296\n",
            "        love     0.7178    0.7577    0.7372      1296\n",
            "    surprise     0.7588    0.8789    0.8144      1296\n",
            "\n",
            "    accuracy                         0.6065      7776\n",
            "   macro avg     0.5915    0.6065    0.5965      7776\n",
            "weighted avg     0.5915    0.6065    0.5965      7776\n",
            "\n",
            "\n",
            "Epoch 05\n",
            "Train Loss: 0.5665 | Acc: 79.07%\n",
            "Val Loss: 1.5667 | Acc: 60.55% | F1: 0.5943\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.4479    0.3380    0.3852      1296\n",
            "       worry     0.4270    0.3542    0.3872      1296\n",
            "   happiness     0.6073    0.6420    0.6242      1296\n",
            "     sadness     0.5821    0.6620    0.6195      1296\n",
            "        love     0.7043    0.7608    0.7315      1296\n",
            "    surprise     0.7674    0.8758    0.8180      1296\n",
            "\n",
            "    accuracy                         0.6055      7776\n",
            "   macro avg     0.5893    0.6055    0.5943      7776\n",
            "weighted avg     0.5893    0.6055    0.5943      7776\n",
            "\n",
            "\n",
            "Epoch 06\n",
            "Train Loss: 0.5657 | Acc: 78.97%\n",
            "Val Loss: 1.5749 | Acc: 60.69% | F1: 0.5981\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.4467    0.3526    0.3941      1296\n",
            "       worry     0.4273    0.3673    0.3950      1296\n",
            "   happiness     0.5980    0.6497    0.6228      1296\n",
            "     sadness     0.5804    0.6543    0.6152      1296\n",
            "        love     0.7316    0.7446    0.7380      1296\n",
            "    surprise     0.7795    0.8727    0.8234      1296\n",
            "\n",
            "    accuracy                         0.6069      7776\n",
            "   macro avg     0.5939    0.6069    0.5981      7776\n",
            "weighted avg     0.5939    0.6069    0.5981      7776\n",
            "\n",
            "\n",
            "Epoch 07\n",
            "Train Loss: 0.5678 | Acc: 78.92%\n",
            "Val Loss: 1.5669 | Acc: 60.71% | F1: 0.5977\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.4479    0.3650    0.4022      1296\n",
            "       worry     0.4276    0.3534    0.3870      1296\n",
            "   happiness     0.6054    0.6451    0.6246      1296\n",
            "     sadness     0.5860    0.6520    0.6172      1296\n",
            "        love     0.7174    0.7523    0.7345      1296\n",
            "    surprise     0.7730    0.8750    0.8208      1296\n",
            "\n",
            "    accuracy                         0.6071      7776\n",
            "   macro avg     0.5929    0.6071    0.5977      7776\n",
            "weighted avg     0.5929    0.6071    0.5977      7776\n",
            "\n",
            "\n",
            "Epoch 08\n",
            "Train Loss: 0.5689 | Acc: 78.86%\n",
            "Val Loss: 1.5684 | Acc: 60.67% | F1: 0.5972\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral     0.4445    0.3711    0.4045      1296\n",
            "       worry     0.4282    0.3449    0.3821      1296\n",
            "   happiness     0.6070    0.6435    0.6247      1296\n",
            "     sadness     0.5869    0.6543    0.6188      1296\n",
            "        love     0.7169    0.7523    0.7342      1296\n",
            "    surprise     0.7702    0.8742    0.8189      1296\n",
            "\n",
            "    accuracy                         0.6067      7776\n",
            "   macro avg     0.5923    0.6067    0.5972      7776\n",
            "weighted avg     0.5923    0.6067    0.5972      7776\n",
            "\n",
            "\n",
            "Early stopping after 6 epochs without improvement\n",
            "\n",
            "Final Test Accuracy: 0.6206\n",
            "\n",
            "Text: OMG just got tickets for the concert!!! üòç #excited\n",
            "Processed: omg got ticket concert smilingfacewithheartey excit\n",
            "Predicted Emotion: happiness (0.67)\n",
            "============================================================\n",
            "\n",
            "Text: This service is terrible! Worst experience ever üò†\n",
            "Processed: servic terribl worst experi ever angryfac\n",
            "Predicted Emotion: worry (0.95)\n",
            "============================================================\n",
            "\n",
            "Text: Feeling so anxious about the interview tomorrow...\n",
            "Processed: feel anxiou interview tomorrow\n",
            "Predicted Emotion: worry (0.79)\n",
            "============================================================\n",
            "\n",
            "Text: Lost my pet today. I'm completely heartbroken üíî\n",
            "Processed: lost pet today complet heartbroken brokenheart\n",
            "Predicted Emotion: sadness (1.00)\n",
            "============================================================\n",
            "\n",
            "Text: What a beautiful morning! üåû #blessed\n",
            "Processed: beauti morn sunwithfac bless\n",
            "Predicted Emotion: happiness (0.48)\n",
            "============================================================\n",
            "\n",
            "Text: lol that's hilarious üòÇ\n",
            "Processed: lol that' hilari facewithtearsofjoy\n",
            "Predicted Emotion: neutral (0.42)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import emoji\n",
        "import kagglehub\n",
        "\n",
        "nltk.download(['punkt', 'stopwords', 'wordnet', 'omw-1.4'], quiet=True)\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "class EnhancedVocab(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.unk_vector = None\n",
        "        self.unk_std = None\n",
        "\n",
        "def twitter_preprocessor(text):\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "    text = re.sub(r'http\\S+|@\\w+', '', text)\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s']\", '', text)\n",
        "    text = re.sub(r'(.)\\1{3,}', r'\\1', text)\n",
        "\n",
        "    tokens = tokenizer.tokenize(text.lower())\n",
        "    processed_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in STOPWORDS and len(token) > 2:\n",
        "            lem = lemmatizer.lemmatize(token)\n",
        "            stem = stemmer.stem(lem)\n",
        "            processed_tokens.append(stem)\n",
        "\n",
        "    return ' '.join(processed_tokens) if processed_tokens else 'neutral'\n",
        "\n",
        "dataset_path = kagglehub.dataset_download(\"pashupatigupta/emotion-detection-from-text\")\n",
        "data = pd.read_csv(os.path.join(dataset_path, \"tweet_emotions.csv\"))\n",
        "data['clean_text'] = data['content'].apply(twitter_preprocessor)\n",
        "\n",
        "top_labels = data['sentiment'].value_counts().nlargest(6).index\n",
        "data = data[data['sentiment'].isin(top_labels)].copy()\n",
        "final_label_mapping = {label: idx for idx, label in enumerate(top_labels)}\n",
        "data['label'] = data['sentiment'].map(final_label_mapping)\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_res, y_res = ros.fit_resample(data[['clean_text']], data['label'])\n",
        "data_balanced = pd.DataFrame({'clean_text': X_res['clean_text'], 'label': y_res})\n",
        "\n",
        "GLOVE_DIM = 100\n",
        "GLOVE_FILE = f\"glove.twitter.27B.{GLOVE_DIM}d.txt\"\n",
        "\n",
        "if not os.path.exists(GLOVE_FILE):\n",
        "    print(\"Downloading GloVe embeddings...\")\n",
        "    os.system(\"wget https://nlp.stanford.edu/data/glove.twitter.27B.zip\")\n",
        "    os.system(\"unzip -o glove.twitter.27B.zip\")\n",
        "\n",
        "print(f\"Loading GloVe ({GLOVE_DIM}d)...\")\n",
        "glove_embeddings = {}\n",
        "valid_vectors = []\n",
        "\n",
        "with open(GLOVE_FILE, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            if len(vector) == GLOVE_DIM:\n",
        "                glove_embeddings[word] = vector\n",
        "                valid_vectors.append(vector)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "all_vectors = np.array(valid_vectors)\n",
        "\n",
        "def build_twitter_vocab(texts, min_freq=2):\n",
        "    counter = Counter()\n",
        "    for text in texts:\n",
        "        counter.update(text.split())\n",
        "\n",
        "    mean_vector = np.mean(all_vectors, axis=0)\n",
        "    std_vector = np.std(all_vectors, axis=0)\n",
        "\n",
        "    vocab = EnhancedVocab()\n",
        "    vocab.update({\n",
        "        \"[PAD]\": 0,\n",
        "        \"[UNK]\": 1,\n",
        "        \"[CLS]\": 2\n",
        "    })\n",
        "\n",
        "    current_idx = 3\n",
        "    for word, freq in counter.items():\n",
        "        if freq >= min_freq and word not in vocab:\n",
        "            vocab[word] = current_idx\n",
        "            current_idx += 1\n",
        "\n",
        "    vocab.unk_vector = mean_vector\n",
        "    vocab.unk_std = std_vector\n",
        "\n",
        "    return vocab\n",
        "\n",
        "vocab = build_twitter_vocab(data_balanced['clean_text'], min_freq=2)\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, GLOVE_DIM))\n",
        "for word, idx in vocab.items():\n",
        "    if word in glove_embeddings:\n",
        "        embedding_matrix[idx] = glove_embeddings[word]\n",
        "    elif idx not in [0, 1, 2]:\n",
        "        embedding_matrix[idx] = np.random.normal(\n",
        "            loc=vocab.unk_vector,\n",
        "            scale=vocab.unk_std,\n",
        "            size=(GLOVE_DIM,)\n",
        "        )\n",
        "\n",
        "seq_lengths = data_balanced['clean_text'].apply(lambda x: len(x.split()))\n",
        "MAX_LEN = int(np.percentile(seq_lengths, 95))\n",
        "print(f\"Optimal Sequence Length: {MAX_LEN}\")\n",
        "\n",
        "def text_to_sequence(text, vocab, max_len=MAX_LEN):\n",
        "    tokens = text.split()[:max_len-1]\n",
        "    sequence = [vocab[\"[CLS]\"]] + [\n",
        "        vocab.get(token, vocab[\"[UNK]\"]) for token in tokens\n",
        "    ]\n",
        "    padding = [vocab[\"[PAD]\"]] * (max_len - len(sequence))\n",
        "    return sequence + padding if len(sequence) < max_len else sequence[:max_len]\n",
        "\n",
        "data_balanced['sequence'] = data_balanced['clean_text'].apply(\n",
        "    lambda x: text_to_sequence(x, vocab, MAX_LEN)\n",
        ")\n",
        "\n",
        "X_train_valid, X_test, y_train_valid, y_test = train_test_split(\n",
        "    np.stack(data_balanced['sequence']),\n",
        "    data_balanced['label'].values,\n",
        "    test_size=0.15,\n",
        "    stratify=data_balanced['label'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_valid,\n",
        "    y_train_valid,\n",
        "    test_size=0.1765,\n",
        "    stratify=y_train_valid,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "class TweetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sequences, labels, augment=False):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.sequences[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.augment and np.random.rand() > 0.5:\n",
        "            mask = np.random.rand(len(seq)) > 0.1\n",
        "            seq = seq[mask]\n",
        "            seq = np.pad(seq, (0, MAX_LEN - len(seq)),\n",
        "                         mode='constant', constant_values=vocab[\"[PAD]\"])\n",
        "        return {\n",
        "            'sequence': torch.tensor(seq, dtype=torch.long),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_dataset = TweetDataset(X_train, y_train, augment=True)\n",
        "valid_dataset = TweetDataset(X_valid, y_valid)\n",
        "test_dataset = TweetDataset(X_test, y_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True\n",
        ")\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True\n",
        ")\n",
        "\n",
        "class EnhancedTwitterTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=256,\n",
        "                 num_layers=4, num_heads=4, num_classes=6,\n",
        "                 max_len=MAX_LEN, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            vocab_size, embedding_dim, padding_idx=vocab[\"[PAD]\"]\n",
        "        )\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = True  # ÿßŸÖ⁄©ÿßŸÜ ŸÅÿß€åŸÜ‚Äåÿ™€åŸàŸÜ€åŸÜ⁄Ø ÿ¨ÿ≤ÿ¶€å\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_len, embedding_dim))\n",
        "        nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            d_model=embedding_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            encoder_layers,\n",
        "            num_layers=num_layers,\n",
        "            norm=nn.LayerNorm(embedding_dim)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(embedding_dim),\n",
        "            nn.Linear(embedding_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout/2),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            nn.init.constant_(module.bias, 0)\n",
        "            nn.init.constant_(module.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size()\n",
        "\n",
        "        token_emb = self.embedding(x)\n",
        "        pos_emb = self.pos_embedding[:, :seq_len, :]\n",
        "        x = self.dropout(token_emb + pos_emb)\n",
        "\n",
        "        padding_mask = (x == self.embedding.weight[vocab[\"[PAD]\"]]).all(dim=-1)\n",
        "        x = self.encoder(x, src_key_padding_mask=padding_mask)\n",
        "\n",
        "        cls_output = x[:, 0, :]\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_SAVE_PATH = \"twitter_emotion_model.pth\"\n",
        "\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    print(\"Loading pre-trained model...\")\n",
        "    model = EnhancedTwitterTransformer(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=GLOVE_DIM,\n",
        "        hidden_dim=256,\n",
        "        num_layers=4,\n",
        "        num_heads=4,\n",
        "        num_classes=len(final_label_mapping),\n",
        "        max_len=MAX_LEN,\n",
        "        dropout=0.2\n",
        "    ).to(device)\n",
        "    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
        "else:\n",
        "    print(\"Initializing new model...\")\n",
        "    model = EnhancedTwitterTransformer(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=GLOVE_DIM,\n",
        "        hidden_dim=256,\n",
        "        num_layers=4,\n",
        "        num_heads=4,\n",
        "        num_classes=len(final_label_mapping),\n",
        "        max_len=MAX_LEN,\n",
        "        dropout=0.2\n",
        "    ).to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
        "\n",
        "class_counts = np.bincount(data_balanced['label'])\n",
        "class_weights = torch.tensor(1. / np.sqrt(class_counts), dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='max',\n",
        "    patience=3,\n",
        "    factor=0.2,\n",
        "    verbose=True,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "best_f1 = 0\n",
        "patience = 6\n",
        "no_improve = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        sequences = batch['sequence'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(sequences)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_acc = 100. * correct / total\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    val_loss = 0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            sequences = batch['sequence'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += predicted.eq(labels).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_acc = 100. * correct_val / total_val\n",
        "    val_loss /= len(valid_loader)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    scheduler.step(f1)\n",
        "\n",
        "    print(f'\\nEpoch {epoch+1:02}')\n",
        "    print(f'Train Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%')\n",
        "    print(f'Val Loss: {val_loss:.4f} | Acc: {val_acc:.2f}% | F1: {f1:.4f}')\n",
        "    print(classification_report(\n",
        "        all_labels, all_preds,\n",
        "        target_names=final_label_mapping.keys(),\n",
        "        digits=4\n",
        "    ))\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        no_improve = 0\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "        print(f\"Model saved with F1: {f1:.4f}\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(f\"\\nEarly stopping after {patience} epochs without improvement\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        sequences = batch['sequence'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(sequences)\n",
        "        _, predicted = outputs.max(1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f'\\nFinal Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "inv_label_mapping = {v: k for k, v in final_label_mapping.items()}\n",
        "\n",
        "test_samples = [\n",
        "    \"OMG just got tickets for the concert!!! üòç #excited\",\n",
        "    \"This service is terrible! Worst experience ever üò†\",\n",
        "    \"Feeling so anxious about the interview tomorrow...\",\n",
        "    \"Lost my pet today. I'm completely heartbroken üíî\",\n",
        "    \"What a beautiful morning! üåû #blessed\",\n",
        "    \"lol that's hilarious üòÇ\"\n",
        "]\n",
        "\n",
        "for text in test_samples:\n",
        "    processed = twitter_preprocessor(text)\n",
        "    seq = text_to_sequence(processed, vocab, MAX_LEN)\n",
        "    seq_tensor = torch.tensor(seq).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(seq_tensor)\n",
        "        prob = torch.softmax(output, dim=1)\n",
        "        pred = torch.argmax(prob).item()\n",
        "\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(f\"Processed: {processed}\")\n",
        "    print(f\"Predicted Emotion: {inv_label_mapping[pred]} ({prob.max().item():.2f})\")\n",
        "    print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
